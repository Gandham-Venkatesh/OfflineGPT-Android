# ğŸ“± TinyLLaMA Offline GPT â€“ Android + Termux Project

> ğŸš€ **Revolutionizing Offline AI for Android** â€“ This is one of the first-ever Android applications fully integrated with an **offline AI model** using **TinyLLaMA**, all powered through **Termux** without any internet dependency after setup!

---

## ğŸŒŸ Project Highlights

- âœ… Completely **offline** AI chatbot (no internet required after setup)
- âœ… Frontend in Android (Java)
- âœ… Backend using **Python + FastAPI** in **Ubuntu (Termux)**
- âœ… Model: **TinyLLaMA 1.1B Chat** (GGUF format)
- âœ… Integration with `llama.cpp` + `llama-cli`
- âœ… Dynamic token-based timer system
- âœ… One-click server start using `start_server.sh`



---

## ğŸ—ï¸ Project Structure

```
TinyLLaMA_OfflineGPT/
â”œâ”€â”€ Android_Frontend/               # Android Studio project
â”œâ”€â”€ Termux_Backend/                # Python + FastAPI backend
â”‚   â”œâ”€â”€ models/                    # GGUF model file (TinyLLaMA)
â”‚   â”œâ”€â”€ llama.cpp/                 # Llama.cpp repo + llama-cli
â”‚   â”œâ”€â”€ tinyllama_api.py          # FastAPI backend
â”‚   â”œâ”€â”€ requirements.txt          # Required Python libraries
â”‚   â”œâ”€â”€ run_instructions.txt      # All setup steps
â”‚   â””â”€â”€ start_server.sh           # One-command server starter
```

---
## ğŸ“¸ Screenshots

Here are a few glimpses of the project in action:

### ğŸ”¹ Android App UI

![App UI Screenshot](./assets/ui-screenshot.jpeg)

### ğŸ”¹ Terminal Setup

![Termux Server Screenshot](./assets/terminal-screenshot.jpeg)

---

## ğŸ”§ Installation Steps

### ğŸ”¹ FRONTEND (Android App)

1. Open `Android_Frontend` in Android Studio.
2. Build the app.
3. App UI contains:
    - User Prompt Section
    - Token size selection: Basic (100), Medium (300), Large (800)
    - Submit button + Visual countdown timer
    - Response tab to view AI response

---

### ğŸ”¹ BACKEND (Termux + Python + FastAPI)

#### ğŸ§ª 1. Install Termux

```bash
pkg update && pkg upgrade
pkg install git python proot-distro
```

#### ğŸ§ 2. Install Ubuntu in Termux

```bash
proot-distro install ubuntu
```

#### ğŸ§‘â€ğŸ’» 3. Login to Ubuntu

```bash
proot-distro login ubuntu
```

#### ğŸ 4. Inside Ubuntu: Install Python & setup virtual environment

```bash
apt update && apt install python3 python3-pip git
python3 -m venv venv
source venv/bin/activate
```

#### ğŸ“ 5. Clone llama.cpp and install requirements

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
pip install -r ../requirements.txt
```

#### ğŸ§  6. Download TinyLLaMA Model

1. First, create a Hugging Face account and generate your Access Token here:
   https://huggingface.co/settings/tokens
2. Use your HuggingFace **Access Token** to download:

```bash
cd ../models

wget --header="Authorization: Bearer YOUR_HF_TOKEN" \
https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

```

#### ğŸ§  7. Create FastAPI file (already added: `tinyllama_api.py`)

Runs llama-cli and returns model response via FastAPI.

#### ğŸ”¥ 8. Start the server manually

```bash
uvicorn tinyllama_api:app --host 0.0.0.0 --port 8000
```

Or just use:

```bash
./start_server.sh
```

---

## ğŸ§ª Testing the App

- Ensure backend is running.
- Install app on Android device.
- Enter prompt, choose token size, submit.
- Wait for visual timer to complete.
- Response appears in the response tab.

---

## ğŸ“Œ Final Notes

- This project shows **how lightweight LLaMA models can empower Android devices** offline.
- Ideal for educational use, low-resource environments, or personal assistant tools without any server cost.
- Can be extended with on-device model loading in future using NDK/C++ or Llama.cpp ports.

---

## ğŸ§  Credits

- [TinyLLaMA](https://huggingface.co/cognitivecomputations/TinyLlama-1.1B-Chat-v1.0-GGUF)
- [Llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Termux](https://termux.dev/)

---

## ğŸ™Œ Support & Contributions

Open to improvements, PRs, or even internship tasks!

---

Made with â¤ï¸ by Gandham Venkatesh



